1. 爬虫是什么
   一段自动抓取互联网信息的程序。
   互联网 - 爬虫 - 价值数据 - 为我所用（新闻聚合阅读器、最爆笑故事APP、美女图片网、图书价格对比网等等）

2. 简单爬虫架构：
   爬虫调度端： 启动爬虫、停止爬虫、监视爬虫运行情况
   
   URL管理器： 对将要爬取的URL和已经爬取的URL进行管理
   网页下载器： 从URL管理器中取出待下载的url，从该url下载网页，存储成字符串，传送给网页解析器
   网页解析器： 一方面解析出有价值的数据，另一方面，每个网页中都有很多url，这些url解析出来后可以补充进入URL管理器。

   URL管理器、网页下载器、网页解析器，三个模块形成循环，只要有相关联的url，就会一直运行下去，直到将互联网上相关联的网页都下载下来。

   简单爬虫架构运行流程：
   1. 调度器询问URL管理器，有没有待爬取的URL。
   2. URL管理器返回是或否。
   3. 如果是的话，调度器从URL管理器中取出一个待爬取的URL。
   4. URL管理器将这个URL返回给调度器。
   5. 得到了URL，调度器将URL传送给下载器，对网页进行下载。
   6. 下载好后，下载器将网页内容传送给调度器。
   7. 然后，调度器会将URL内容传送给解析器，进行解析。
   8. 解析之后，解析器会将价值数据和新URL列表传送给调度器。
   9. 一方面，调度器会将价值数据传送给应用，进行数据收集。另一方面，会将新的URL列表补充进URL管理器。
   10. 这时，如果URL管理器还有未爬取的URL，那个这个循环会一直运行下去，直到爬取完所有没有爬取过的URL。
   11. 最后，调度器会调用应用的方法进行输出，将价值数据输出到我们希望的格式。

3. URL管理器
   管理待抓取的URL集合和已爬取的URL集合。避免重复抓取和循环抓取。

   支持功能：
   1. 添加新URL到待爬取集合中
   2. 在添加的同时，需要判断待添加URL是否在容器中已经存在了，如果是的话，就不能添加，防止重复抓取。
   3. 获取待爬取的URL
   4. 获取时需要判断窗口中是否还有待爬取的URL
   5. 如果URL被爬取之后，需要将这个URL从待爬取集合移动到已爬取集合。

   实现方式：
   1. 将待爬取URL和已爬取URL存储在内存中。python中使用两个set()存储，只要set()可以直接去除集合中重复的元素。
   2. 将URL存储在关系型数据库中，如MYSQL。创建一个表urls，里面有两个字段，url和is_crawled，url用于表示url，is_crawled表示是否已爬取。
   3. 将url存储在缓存数据库中，如redis。redis本身就支持set()数据结构，这样就可以将待爬取的URL和已爬取的URL存储在两个set()中。目前，大型的互联网公司，因为缓存数据库的高性能，所以都将url存储在缓存的数据库中。对于个人开发者来说，可以存储在内存中，如果内存不够用，或者想永久保存，可以存储在关系型数据库中。

4. 网页下载器
   将互联网上URL对应的网页下载到本地，是爬虫的核心组件。

   网页下载器类似于浏览器，会将URL地址对应的网页，以HTML的方式下载到本地，存储成一个本地文件或内存字符串，然后才能进行后续的分析处理。 

   python有哪几种网页下载器：
   1. urllib2: python官方的基础模块，支持直接url下载、以及向网页提交一些用户输入的数据、需要登录网页的cokie处理、需要代理访问的代理处理等增强功能。
   2. requests: 第三方插件，提供更加庞大的功能。

   urllib2下载网页的三种方法：
   1. 最简洁的方法： urllib2.urlopen(url)
      import urllib2

      # 直接请求
      response = urllib2.urlopen('http://www.baidu.com')

      # 获取状态码，如果是200表示获取成功
      print response.getcode()

      # 读取内容
      cont = response.read()
   2. 增强处理
      添加一个header，向服务器提交http的头信息
      添加一个data，向服务器提交用户输入的信息
      现在，有3个参数，url、header、data，将这三个参数传送给urllib2.Request()，生成一个Request对象，然后依然使用urllib2.urlopen(request)方法，以request作为参数，发送网络请求。

      import urllib2

      # 创建Request对象
      request = urllib2.Request(url)

      # 添加数据
      request.add_data('a', '1')
      #添加http的header
      request.add_header('User-Agent', 'Mozilla/5.0')

      # 发送请求获取结果
      response = urllib2.urlopen(request)
   3. 添加特殊情景的处理器
      有些网页需要用户登录才能够访问，需要添加cookie的处理，相应地使用HTTPCookieProcessor
      有些网页需要代理才能访问，使用ProxyHandler
      还有些网页是使用https加密访问的，使用HTTPSHandler
      有些网页它们的url存在相互的自动的跳转关系，使用HTTPRedirectHandler
      这些参数，将它们传递给urllib2.build_opener(handler)方法，生成opener对象
      然后，给urllib2.install_opener(opener)这个对象，这样urllib2就具有了这些情景的处理能力
      然后，依然使用urllib2.urlopen(url)或者urllib2.urlopen(request)

      import urllib2, cookielib

      # 创建cookie容器
      cj = cookielib.CookieJar()

      # 创建1个opener
      opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))

      # 给urllib2安装opener
      urllib2.install_opener(opener)

      # 使用带有cookie的urllib2访问网页
      response = urllib2.urlopen(url)

5. 网页解析器
   从网页中提供出有价值数据

   python中的网页解析器有：
   1. 正则表达式：
      会将整个网页当成一个字符串，使用模糊匹配的方式，提取有价值的数据。
      虽然直观，但是如果文档比较复杂的话，这种方式非常麻烦。
   2. python自动的html.parser模块
   3. BeautifulSoup这个第三方插件
      可以使用html.parser或lxml作为自己的解析器，推荐
   4. lxml解析html或xml文件

   第一种解析方法中字符串式的模糊匹配，其它三种方式是结构化解析
   结构化解析（DOM）树，是将整个网页文档加载成一个DOM树，以树的结构来进行上下级的元素的遍历和访问。

6. beautifulsoup
   1. 安装
      pip install beautifulsoup4
   2. 语法
      1. 根据一个下载好的html网页的字符串，创建一个BeautifulSoup对象，创建这个对象的同时，就将整个文档字符串加载成一个字符串。
      2. 根据dom树，进行各种节点的搜索(按节点名称、节点属性、节点文字)
         find_all, 搜索出所有满足条件的节点
         find，搜索出第一个满足要求的节点
      3. 得到节点以后，就可以访问节点的名称、属性、文字
   3. 例子
      from bs4 import BeautifulSoup

      # 根据HTML网页字符串创建BeautifulSoup对象
      soup = BeautifulSoup(
                          html_doc,               # HTML文档字符串
                          'html.parser',          # HTML解析器
                          from_encoding='utf8'    # HTML文档的编码
                          )
      # 搜索方法: find_all(name, attrs, string)
      # 查找所有标签为a的节点
      soup.find_all('a')
      # 查找所有标签为a，链接符合/view/123.htm形式的节点
      soup.find_all('a', href='/view/123.htm')
      soup.find_all('a', href=re.compile(r'/view/\d+\.htm'))

      # 查找所有标签为div, class为abc，文字为python的节点
      soup.find_all('div', class_='abc', string='Python')

      # 得到节点: <a href='1.htm'>Python</a>
      # 获取查找到的节点的标签名称
      node.name
      # 获取查找到的a节点的href属性
      node['href']
      # 获取查找到的a节点的链接文字
      node.get_text()