### 爬虫知识大纲

1. 如何抓取html页面  
   http请求的处理，urllib、urllib2、requests  
   处理后的请求可以模拟浏览器发送请求，获取服务器响应的文件  

2. 解析服务器响应的内容  
   文本解析：re、xpath(使用最广泛)、beautifulSoup4  
   json解析：jsonpath  
   其它：pyquery等  
   使用某种描述性语言给我们需要提取的数据定义一个匹配规则，符合这个规则的数据就会被匹配  
   
3. 如何采集动态html、验证码的处理  
   通用的动态页面采集：Selenium + PhantomJS(无界面浏览器)：模拟真实浏览器加载js、ajax等非静态页面数据  
   Tesseract：机器学习库，机器图像识别系统，可以处理简单的验证码，复杂的验证码可以通过手动输入，或者专门的打码平台  

4. Scrapy框架：(Scrapy、Pyspider)  
   高定制性、高性能(异步网络框架twisted)、所以数据下载速度非常快，提供了数据存储、数据下载、提取规则等组件  

5. 分布式策略  
   scrapy redis，在scrapy的基础上添加了一套以redis数据库为核心的组件，让scrapy框架支持分布式功能，主要在redis里做请求指纹去重、请求分配、数据临时存储。  

6. 爬虫、反爬虫的斗争  
   user agent、代理、验证码、动态数据加载、加密数据  

### 爬虫分类
通用爬虫和聚集爬虫  

通用爬虫：搜索引擎用的爬虫系统。  
   目标是尽可能把互联网上所有的网页下载下来，放到本地服务器形成备份，再对这些网页做相关处理，提取关键字、去掉广告、最后提供一个用户检索接口。  
   爬取流程：  
       1. 首先选取一部分已有的url，把这些url放到待爬取队列  
       2. 从队列里取出这些url，然后解析dns得到主机ip，然后去这个ip对应的服务器下载html页面，保存到搜索引擎的本地数据服务里，之后把爬过的url放入已爬取队列  
       3. 分析这些网页内容，找出网页里其它url链接，继续执行第二步  

聚集爬虫：针对某种内容的爬虫  

